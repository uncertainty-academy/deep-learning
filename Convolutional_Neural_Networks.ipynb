{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NcAHF4g8Xa93"
   },
   "source": [
    "# Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFOuMk56XjC8"
   },
   "source": [
    "## Introduction\n",
    "In this practical we will cover the basics of convolutional neural networks, or \"ConvNets\". ConvNets were invented in the late 1980s/early 1990s, and have had tremendous success especially with vision (although they have also been used very successfully in speech processing pipelines, and more recently, for machine translation).\n",
    "\n",
    "## Learning Objectives\n",
    "* Be able to explain what a convolutional layer does and how it's different from a fully-connected layer \n",
    "* Understand  the assumptions and trade-offs that are being made when using convolutional architectures\n",
    "* Be able to build a convolutional architecture using Tensorflow and Keras Layers\n",
    "* Be able to use Keras to train a model on a dataset\n",
    "* Implement either batch normalisation or a very small residual network\n",
    "\n",
    "## Running on GPU\n",
    "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelelator\" box. This is all you need to do, Colab and Tensorflow will take care of the rest! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bJG0K_T7wifE",
    "outputId": "e9f03e31-ebd3-43b4-c4ab-6fb3aff60925"
   },
   "outputs": [],
   "source": [
    "#@title Imports (RUN ME!) { display-mode: \"form\" }\n",
    "\n",
    "# TODO: Swallow output\n",
    "!pip -q install pydot_ng\n",
    "!pip -q install graphviz\n",
    "!apt install graphviz > /dev/null\n",
    "\n",
    "# from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "try:\n",
    "  tf.enable_eager_execution()\n",
    "  print('Running in Eager mode.')\n",
    "except ValueError:\n",
    "  print('Already running in Eager mode')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vOp67cfpaTlC"
   },
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6JbXSVKulDoD"
   },
   "source": [
    "A convolutional layer maps an input *volume* (meaning, a 3-D input tensor, e.g. [width, height, channels]) to an output volume through a set of learnable filters, which make up the parameters of the layer. Every filter is small spatially (along width and height), but extends through the full depth of the input volume. (Eg: A filter in the first layer of a ConvNet might have size [5, 5, 3]). During the forward pass, we convolve (\"slide\") each filter across the width and height of the input volume and compute element-wise dot products between the entries of the filter and the input at any position. As we slide the filter over the width and height of the input volume we will produce a 2-dimensional activation map that gives the responses of that filter at every spatial position. Each convolutional layer will have such a set of filters, and each of them will produce a separate 2-dimensional activation map. We then stack these activation maps along the depth-dimension to produce the output volume.\n",
    "\n",
    "By using these filters which map to a small sub-volume of the input, we can to a large extent,control the parameter explosion that we would get with a (fully-connected) feed-forward network. This **parameter sharing** actually also tends to improve the performance of the model on inputs like natural images because it provides the model with some limited **translation invariance**. Translation invariance means that if the image (or a feature in the image) is translated (moved), the model will not be significantly affected. Think about why this is the case!\n",
    "\n",
    "The following animation illustrates these ideas, make sure you understand them!\n",
    "\n",
    "![Convolution Animation](https://miro.medium.com/max/588/1*BMngs93_rm2_BpJFH2mS0Q.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eLiuT6TcmXw8"
   },
   "source": [
    "The hyper-parameters of a convolutional layer are as follows:\n",
    "* **Filters** defines the number of filters in the layer\n",
    "* **Kernel Size** defines the width and height of the filters (also called \"kernels\") in the layer. Note that kernels always have the same depth as the inputs to the layer.\n",
    "* **Stride** defines the number of pixels by which we move the filter when \"sliding\" it along the input volume. Typically this value would be 1, but values of 2 and 3 are also sometimes used.\n",
    "* **Padding** refers to the addition of 0-value pixels to the edges of the input volume along the width and height dimensions. In Tensorflow you can set this to \"VALID\", which essentially does no padding or \"SAME\" which pads the input such that the output width and height are the same as the input.\n",
    "\n",
    "Lets look at a very simple, dummy example to see how the values of the hyper-parameters affect the output size of a convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "iluvPEPInWKn",
    "outputId": "7525b2e3-0e06-4d25-925a-fbc827404625"
   },
   "outputs": [],
   "source": [
    "# Create a random colour \"image\" of shape 10x10 with a depth of 3 (for red, green and blue)\n",
    "dummy_input = np.random.uniform(size=[10, 10, 3])\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.imshow(dummy_input)\n",
    "ax.grid(False)\n",
    "print('Input shape: {}'.format(dummy_input.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZU1gD5hnsHbc"
   },
   "source": [
    "Now adjust the hyperparameters using the sliders on the right and see how the output shape changes for a [10, 10, 3] input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "OF_KxVpSpE6y",
    "outputId": "6fab73db-7f70-4164-84ec-0bec59808f75"
   },
   "outputs": [],
   "source": [
    "#@title Convolutional layer parameters {run: \"auto\"}\n",
    "filters = 3  #@param { type: \"slider\", min:0, max: 10, step: 1 }\n",
    "kernel_size = 2 #@param { type: \"slider\", min:1, max: 10, step: 1 }\n",
    "stride = 1 #@param { type: \"slider\", min:1, max: 3, step: 1 }\n",
    "\n",
    "conv_layer = tf.keras.layers.Conv2D(\n",
    "    filters=filters, \n",
    "    kernel_size=kernel_size, \n",
    "    strides=stride,\n",
    "    padding=\"valid\",\n",
    "    input_shape=[10, 10, 3])\n",
    "\n",
    "# Convert the image to a tensor and add an extra batch dimension which\n",
    "# the convolutional layer expects.\n",
    "input_tensor = tf.convert_to_tensor(dummy_input[None, :, :, :])\n",
    "convoluted = conv_layer(input_tensor)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.imshow(np.squeeze(convoluted.numpy()))\n",
    "ax.grid(False)\n",
    "print('The output dimension is:')\n",
    "list([d.value for d in convoluted.shape])[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dzh-9TL1sMCT"
   },
   "source": [
    "Note especially how output width and height are related to ```kernel_size``` and ```stride```, and how the output depth is related to ```filters```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4Vsrgyudd2E"
   },
   "source": [
    "## The CIFAR10 Dataset\n",
    "Now that we understand convolutional, max-pooling and feed-forward layers, we can combine these as building block to build a ConvNet classifier for images. For this practical, we will use the colour image dataset CIFAR10 (pronounced \"seefar ten\") which consists of 50,000 training images and 10,000 test images. As we did in Practical 1, we take 10,000 images from the training set to form a validation set and visualise some example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "flWYFg3ydvMU",
    "outputId": "fde0791b-3017-4ff8-992f-8c56235eb469"
   },
   "outputs": [],
   "source": [
    "cifar = tf.keras.datasets.cifar10\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar.load_data()\n",
    "cifar_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QSzdYWpZd8RE"
   },
   "outputs": [],
   "source": [
    "# Take the last 10000 images from the training set to form a validation set \n",
    "train_labels = train_labels.squeeze()\n",
    "validation_images = train_images[-10000:, :, :]\n",
    "validation_labels = train_labels[-10000:]\n",
    "train_images = train_images[:-10000, :, :]\n",
    "train_labels = train_labels[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x8jf1myGQP1O"
   },
   "source": [
    "What are the shapes and data-types of train_images and train_labels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "wzLutGn-P7mg",
    "outputId": "6106a826-e509-43ca-f75f-2245feae6601"
   },
   "outputs": [],
   "source": [
    "print('train_images.shape = {}, data-type = {}'.format(train_images.shape, train_images.dtype))\n",
    "print('train_labels.shape = {}, data-type = {}'.format(train_labels.shape, train_labels.dtype))\n",
    "\n",
    "print('validation_images.shape = {}, data-type = {}'.format(validation_images.shape, validation_images.dtype))\n",
    "print('validation_labels.shape = {}, data-type = {}'.format(validation_labels.shape, validation_labels.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynwzGIAneBbb"
   },
   "source": [
    "### Visualise examples from the dataset\n",
    "Run the cell below multiple times to see various images. (They might look a bit blurry because we've blown up the small images.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "8nMTxCOjd9WW",
    "outputId": "58a6e003-6b73-414c-80f3-2c7e2a79aade"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid('off')\n",
    "\n",
    "  img_index = np.random.randint(0, 40000)\n",
    "  plt.imshow(train_images[img_index])\n",
    "  plt.xlabel(cifar_labels[train_labels[img_index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wN-XUzp-fpyp"
   },
   "source": [
    "## A ConvNet Classifier\n",
    "Finally, we build a simple convolutional architecture to classify the CIFAR images. We will build a mini version of the AlexNet architecture, which consists of 5 convolutional layers with max-pooling, followed by 3 fully-connected layers at the end. In order to investigate the effect each of these two layers have on the number of parameters, we'll build the model in two stages. \n",
    "\n",
    "First, the convolutional layers + max-pooling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9zloewLws0b"
   },
   "outputs": [],
   "source": [
    "# Define the convolutinal part of the model architecture using Keras Layers.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=48, kernel_size=(3, 3), activation=tf.nn.relu, input_shape=(32, 32, 3), padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "    tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation=tf.nn.relu, padding='same'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(3, 3)),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHoTJ0XTgWKN"
   },
   "source": [
    "How many parameters are there in the convolutional part of the architecture? We can easily inspect this using the model summary function in Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbJAaisIgZyX"
   },
   "source": [
    "Now we add a fully-connected part. Note that we also add \"Dropout\" after the first fully-connected layer. Dropout is a regularization technique which randomly zeros out (\"drops\") connections between neurons, and it was one of the key innovations of the AlexNet paper in 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gaHgWaNb1C0W"
   },
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Flatten())  # Flatten \"squeezes\" a 3-D volume down into a single vector.\n",
    "model.add(tf.keras.layers.Dense(1024, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "model.add(tf.keras.layers.Dense(1024, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNMAg7s3W0gg"
   },
   "source": [
    "###Visualizing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCL9cH8a3F1W"
   },
   "source": [
    "Let's build a flow-diagram of the model we've constructed to see how information flows between the different layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFrH0OVORco2"
   },
   "source": [
    "### Training and Validating the model\n",
    "In the last practical we wrote out the dataset pipeline, loss function and training-loop to give you a good appreciation for how it works. This time, we use the training loop built-in to Keras. For simple, standard datasets like CIFAR, doing it this way will work fine, but it's important to know what goes on under the hood because you may need to write some or all of the steps out manually when working with more complex datasets! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "wJ6JAqUL1TDu",
    "outputId": "7177d9c2-041d-40b6-c43a-ce2cdc3c90df"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 10  # The number of epochs (full passes through the data) to train for\n",
    "\n",
    "# Compiling the model adds a loss function, optimiser and metrics to track during training\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# The fit function allows you to fit the compiled model to some training data\n",
    "model.fit(x=train_images, \n",
    "          y=train_labels, \n",
    "          batch_size=batch_size, \n",
    "          epochs=num_epochs, \n",
    "          validation_data=(validation_images, validation_labels.astype(np.float32)))\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "waS75-jNaEL1"
   },
   "source": [
    "### Test performance\n",
    "Finally, we evaluate how well the model does on the held-out test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AVAdjH6o13tQ",
    "outputId": "72e29943-105b-4375-ecb6-fde1264f89be"
   },
   "outputs": [],
   "source": [
    "metric_values = model.evaluate(x=test_images, y=test_labels)\n",
    "\n",
    "print('Final TEST performance')\n",
    "for metric_value, metric_name in zip(metric_values, model.metrics_names):\n",
    "  print('{}: {}'.format(metric_name, metric_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lsBthNB4FlL"
   },
   "source": [
    "Note that we achieved roughly 80% training set accuracy, but our test accuracy is only around 67%. What do you think may be the reason for this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UlrSyMQpoV9f"
   },
   "source": [
    "### Classifying examples\n",
    "We now use our trained model to classify a sample of 25 images from the test set. We pass these 25 images to the  ```model.predict``` function, which returns a [25, 10] dimensional matrix. The entry at position $(i, j)$ of this matrix contains the probability that image $i$ belongs to class $j$. We obtain the most-likely prediction using the ```np.argmax``` function which returns the index of the maximum entry along the columns. Finally, we plot the result with the prediction and prediction probability labelled underneath the image and true label on the side. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjzP384wm9OW"
   },
   "outputs": [],
   "source": [
    "img_indices = np.random.randint(0, len(test_images), size=[25])\n",
    "sample_test_images = test_images[img_indices]\n",
    "sample_test_labels = [cifar_labels[i] for i in test_labels[img_indices].squeeze()]\n",
    "\n",
    "predictions = model.predict(sample_test_images)\n",
    "max_prediction = np.argmax(predictions, axis=1)\n",
    "prediction_probs = np.max(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "Ol-f9SacnySQ",
    "outputId": "cc4d4a4c-2bda-4d86-bf4a-9a5f1f3cd5ad"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i, (img, prediction, prob, true_label) in enumerate(\n",
    "    zip(sample_test_images, max_prediction, prediction_probs, sample_test_labels)):\n",
    "  plt.subplot(5,5,i+1)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "  plt.grid('off')\n",
    "\n",
    "  plt.imshow(img)\n",
    "  plt.xlabel('{} ({:0.3f})'.format(cifar_labels[prediction], prob))\n",
    "  plt.ylabel('{}'.format(true_label))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0618tOoMv0Y"
   },
   "source": [
    "### Question\n",
    "What do you think of the model's predictions? Looking at the model's confidence (the probability assigned to the predicted class), look for examples of the following cases:\n",
    "1. The model was correct with high confidence\n",
    "2. The model was correct with low confidence\n",
    "3. The model was incorrect with high confidence\n",
    "4. The model was incorrect with low confidence\n",
    "\n",
    "What do you think the (relative) loss values would be in those cases? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3bIU8BErhiJ"
   },
   "source": [
    "## Your Tasks\n",
    "1. [**ALL**] Experiment with the network architecture, try changing the numbers, types and sizes of layers, the sizes of filters, using different padding etc. How do these decisions affect the performance of the model? In particular, try building a *fully convolutinoal* network, with no (max-)pooling layers. \n",
    "2. [**ALL**] Implement BATCH NORMALISATION ([Tensorflow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) and [research paper](http://proceedings.mlr.press/v37/ioffe15.pdf)) to improve the model's generalisation.\n",
    "3. [**ADVANCED**] Read about Residual networks ([original paper](https://arxiv.org/pdf/1512.03385.pdf), ) and add **shortcut connections** to the model architecture. Try to build a simple reusable \"residual block\" as a [Keras Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model). \n",
    "4. [**OPTIONAL**]. Visualise the filters of the convolutional layers using Matplotlib. **HINT**: You can retrieve a reference to an indivual layer from the sequential Keras model by calling```model.get_layer(name)```, replacing \"name\" with the name of the layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0e-IdtqUknDK"
   },
   "source": [
    "##Additional Resources\n",
    "\n",
    "Here's some more information on ConvNets:\n",
    "\n",
    "* Chris Colah's blog post on [Understanding Convolutions](https://colah.github.io/posts/2014-07-Understanding-Convolutions/)\n",
    "* [How do convolutional neural networks work?](http://brohrer.github.io/how_convolutional_neural_networks_work.html)\n",
    "* The [CS231n course](https://cs231n.github.io/)\n",
    "* [Building blocks of interpretability](https://distill.pub/2018/building-blocks/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "SFvYLebFZzGG",
    "5uzeJaMB7DPa",
    "py0V6UwC6_kH",
    "OJzCooQO66D3"
   ],
   "name": "Practical 2: Convolutional Neural Networks",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_deep_learning_p3",
   "language": "python",
   "name": "venv_deep_learning_p3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
